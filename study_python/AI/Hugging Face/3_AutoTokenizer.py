from transformers import AutoTokenizer

# AutoTokenizer è‡ªåŠ¨æ ‡è®°å™¨
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
encoding = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
print(encoding)
"""
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 
100, 58263, 13299, 119, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
"""

"""
The tokenizer returns a dictionary containing:
æ ‡è®°å™¨è¿”å›ä¸€ä¸ªåŒ…å«ä»¥ä¸‹å†…å®¹çš„å­—å…¸ï¼š

    input_ids: numerical representations of your tokens.
    ä½ çš„ä»¤ç‰Œçš„æ•°å­—è¡¨ç¤º
    attention_mask: indicates which tokens should be attended to.
    è¡¨ç¤ºåº”è¯¥å…³æ³¨å“ªäº›æ ‡è®°ã€‚

A tokenizer can also accept a list of inputs, 
and pad and truncate the text to return a batch with uniform length:
æ ‡è®°å™¨è¿˜å¯ä»¥æ¥å—è¾“å…¥åˆ—è¡¨ï¼Œå¹¶å¡«å……å’Œæˆªæ–­æ–‡æœ¬ä»¥è¿”å›å…·æœ‰ç»Ÿä¸€é•¿åº¦çš„æ‰¹æ¬¡ï¼š
"""
# Pytorch
pt_batch = tokenizer(
    [
        "We are very happy to show you the ğŸ¤— Transformers library.",
        "We hope you don't hate it.",
    ],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)
print(pt_batch)
"""
{'input_ids': 
tensor([[  101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 
10103, 100, 58263, 13299,   119,   102],
        [  101, 11312, 18763, 10855, 11530,   112,   162, 39487, 10197,   119,
           102,     0,     0,     0]]), 
'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 
'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}
"""
